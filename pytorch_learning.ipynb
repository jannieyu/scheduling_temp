{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   in_degree  out_degree  num_descendants  longest_descendant_path  \\\n",
      "0          0           2                4                        2   \n",
      "1          0           1                2                        1   \n",
      "2          1           1                2                        1   \n",
      "3          2           0                1                        0   \n",
      "4          1           0                1                        0   \n",
      "\n",
      "   in_degree_betweenness_centrality  out_degree_betweenness_centrality  \\\n",
      "0                               0.0                                0.4   \n",
      "1                               0.0                                0.2   \n",
      "2                               0.2                                0.2   \n",
      "3                               0.4                                0.0   \n",
      "4                               0.2                                0.0   \n",
      "\n",
      "   trophic_levels  \n",
      "0             1.0  \n",
      "1             1.0  \n",
      "2             2.0  \n",
      "3             2.0  \n",
      "4             3.0  \n",
      "      psize\n",
      "0  2.999997\n",
      "1  1.999990\n",
      "2  1.999990\n",
      "3  1.000000\n",
      "4  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Read in training data\n",
    "train_data_df = pd.read_csv(\"train_data.csv\", header=0, index_col=0)\n",
    "train_labels_df = pd.read_csv(\"train_labels.csv\", header=0, index_col=0)\n",
    "print(train_data_df.head())\n",
    "print(train_labels_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1301\n",
      "[[0.  2.  4.  2.  0.  0.4 1. ]\n",
      " [0.  1.  2.  1.  0.  0.2 1. ]]\n",
      "1301\n",
      "[[2.9999972 ]\n",
      " [1.99998992]]\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "train_data = train_data_df.to_numpy()\n",
    "train_labels = train_labels_df.to_numpy()\n",
    "\n",
    "print(len(train_data))\n",
    "print(train_data[:2])\n",
    "print(len(train_labels))\n",
    "print(train_labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(\n",
      "  (linear): Linear(in_features=7, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters and initializations\n",
    "input_dim = 7\n",
    "output_dim = 1\n",
    "lr = 0.01\n",
    "epochs = 100\n",
    "\n",
    "model = LinearRegression(input_dim, output_dim)\n",
    "model = model.float()\n",
    "criterion1 = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0000],\n",
       "        [2.0000],\n",
       "        [2.0000],\n",
       "        ...,\n",
       "        [2.0000],\n",
       "        [1.0000],\n",
       "        [1.0000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Variable(torch.from_numpy(train_data).float())\n",
    "labels = Variable(torch.from_numpy(train_labels).float())\n",
    "#inputs\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.1181, grad_fn=<MseLossBackward0>)\n",
      "epoch 0, loss 20.11812973022461\n",
      "tensor(11.2661, grad_fn=<MseLossBackward0>)\n",
      "epoch 1, loss 11.266084671020508\n",
      "tensor(7.0691, grad_fn=<MseLossBackward0>)\n",
      "epoch 2, loss 7.069088935852051\n",
      "tensor(5.0749, grad_fn=<MseLossBackward0>)\n",
      "epoch 3, loss 5.074882984161377\n",
      "tensor(4.1233, grad_fn=<MseLossBackward0>)\n",
      "epoch 4, loss 4.123287200927734\n",
      "tensor(3.6654, grad_fn=<MseLossBackward0>)\n",
      "epoch 5, loss 3.6653757095336914\n",
      "tensor(3.4414, grad_fn=<MseLossBackward0>)\n",
      "epoch 6, loss 3.4413998126983643\n",
      "tensor(3.3284, grad_fn=<MseLossBackward0>)\n",
      "epoch 7, loss 3.3284342288970947\n",
      "tensor(3.2683, grad_fn=<MseLossBackward0>)\n",
      "epoch 8, loss 3.2682886123657227\n",
      "tensor(3.2334, grad_fn=<MseLossBackward0>)\n",
      "epoch 9, loss 3.233410120010376\n",
      "tensor(3.2107, grad_fn=<MseLossBackward0>)\n",
      "epoch 10, loss 3.210745334625244\n",
      "tensor(3.1941, grad_fn=<MseLossBackward0>)\n",
      "epoch 11, loss 3.1941025257110596\n",
      "tensor(3.1805, grad_fn=<MseLossBackward0>)\n",
      "epoch 12, loss 3.1805405616760254\n",
      "tensor(3.1687, grad_fn=<MseLossBackward0>)\n",
      "epoch 13, loss 3.168656587600708\n",
      "tensor(3.1578, grad_fn=<MseLossBackward0>)\n",
      "epoch 14, loss 3.157777786254883\n",
      "tensor(3.1476, grad_fn=<MseLossBackward0>)\n",
      "epoch 15, loss 3.1475765705108643\n",
      "tensor(3.1379, grad_fn=<MseLossBackward0>)\n",
      "epoch 16, loss 3.137890100479126\n",
      "tensor(3.1286, grad_fn=<MseLossBackward0>)\n",
      "epoch 17, loss 3.128633737564087\n",
      "tensor(3.1198, grad_fn=<MseLossBackward0>)\n",
      "epoch 18, loss 3.1197588443756104\n",
      "tensor(3.1112, grad_fn=<MseLossBackward0>)\n",
      "epoch 19, loss 3.111237049102783\n",
      "tensor(3.1030, grad_fn=<MseLossBackward0>)\n",
      "epoch 20, loss 3.1030471324920654\n",
      "tensor(3.0952, grad_fn=<MseLossBackward0>)\n",
      "epoch 21, loss 3.095172882080078\n",
      "tensor(3.0876, grad_fn=<MseLossBackward0>)\n",
      "epoch 22, loss 3.0876009464263916\n",
      "tensor(3.0803, grad_fn=<MseLossBackward0>)\n",
      "epoch 23, loss 3.0803186893463135\n",
      "tensor(3.0733, grad_fn=<MseLossBackward0>)\n",
      "epoch 24, loss 3.073314666748047\n",
      "tensor(3.0666, grad_fn=<MseLossBackward0>)\n",
      "epoch 25, loss 3.0665767192840576\n",
      "tensor(3.0601, grad_fn=<MseLossBackward0>)\n",
      "epoch 26, loss 3.060096025466919\n",
      "tensor(3.0539, grad_fn=<MseLossBackward0>)\n",
      "epoch 27, loss 3.0538620948791504\n",
      "tensor(3.0479, grad_fn=<MseLossBackward0>)\n",
      "epoch 28, loss 3.0478649139404297\n",
      "tensor(3.0421, grad_fn=<MseLossBackward0>)\n",
      "epoch 29, loss 3.04209566116333\n",
      "tensor(3.0365, grad_fn=<MseLossBackward0>)\n",
      "epoch 30, loss 3.036545753479004\n",
      "tensor(3.0312, grad_fn=<MseLossBackward0>)\n",
      "epoch 31, loss 3.0312063694000244\n",
      "tensor(3.0261, grad_fn=<MseLossBackward0>)\n",
      "epoch 32, loss 3.026069402694702\n",
      "tensor(3.0211, grad_fn=<MseLossBackward0>)\n",
      "epoch 33, loss 3.0211269855499268\n",
      "tensor(3.0164, grad_fn=<MseLossBackward0>)\n",
      "epoch 34, loss 3.0163722038269043\n",
      "tensor(3.0118, grad_fn=<MseLossBackward0>)\n",
      "epoch 35, loss 3.0117969512939453\n",
      "tensor(3.0074, grad_fn=<MseLossBackward0>)\n",
      "epoch 36, loss 3.0073955059051514\n",
      "tensor(3.0032, grad_fn=<MseLossBackward0>)\n",
      "epoch 37, loss 3.003159523010254\n",
      "tensor(2.9991, grad_fn=<MseLossBackward0>)\n",
      "epoch 38, loss 2.9990837574005127\n",
      "tensor(2.9952, grad_fn=<MseLossBackward0>)\n",
      "epoch 39, loss 2.995161771774292\n",
      "tensor(2.9914, grad_fn=<MseLossBackward0>)\n",
      "epoch 40, loss 2.9913876056671143\n",
      "tensor(2.9878, grad_fn=<MseLossBackward0>)\n",
      "epoch 41, loss 2.987755537033081\n",
      "tensor(2.9843, grad_fn=<MseLossBackward0>)\n",
      "epoch 42, loss 2.984260320663452\n",
      "tensor(2.9809, grad_fn=<MseLossBackward0>)\n",
      "epoch 43, loss 2.980896472930908\n",
      "tensor(2.9777, grad_fn=<MseLossBackward0>)\n",
      "epoch 44, loss 2.977658987045288\n",
      "tensor(2.9745, grad_fn=<MseLossBackward0>)\n",
      "epoch 45, loss 2.9745428562164307\n",
      "tensor(2.9715, grad_fn=<MseLossBackward0>)\n",
      "epoch 46, loss 2.971543550491333\n",
      "tensor(2.9687, grad_fn=<MseLossBackward0>)\n",
      "epoch 47, loss 2.9686567783355713\n",
      "tensor(2.9659, grad_fn=<MseLossBackward0>)\n",
      "epoch 48, loss 2.9658780097961426\n",
      "tensor(2.9632, grad_fn=<MseLossBackward0>)\n",
      "epoch 49, loss 2.963202953338623\n",
      "tensor(2.9606, grad_fn=<MseLossBackward0>)\n",
      "epoch 50, loss 2.960628032684326\n",
      "tensor(2.9581, grad_fn=<MseLossBackward0>)\n",
      "epoch 51, loss 2.9581491947174072\n",
      "tensor(2.9558, grad_fn=<MseLossBackward0>)\n",
      "epoch 52, loss 2.9557626247406006\n",
      "tensor(2.9535, grad_fn=<MseLossBackward0>)\n",
      "epoch 53, loss 2.9534647464752197\n",
      "tensor(2.9513, grad_fn=<MseLossBackward0>)\n",
      "epoch 54, loss 2.9512524604797363\n",
      "tensor(2.9491, grad_fn=<MseLossBackward0>)\n",
      "epoch 55, loss 2.949122190475464\n",
      "tensor(2.9471, grad_fn=<MseLossBackward0>)\n",
      "epoch 56, loss 2.9470713138580322\n",
      "tensor(2.9451, grad_fn=<MseLossBackward0>)\n",
      "epoch 57, loss 2.945096015930176\n",
      "tensor(2.9432, grad_fn=<MseLossBackward0>)\n",
      "epoch 58, loss 2.9431941509246826\n",
      "tensor(2.9414, grad_fn=<MseLossBackward0>)\n",
      "epoch 59, loss 2.9413626194000244\n",
      "tensor(2.9396, grad_fn=<MseLossBackward0>)\n",
      "epoch 60, loss 2.939598321914673\n",
      "tensor(2.9379, grad_fn=<MseLossBackward0>)\n",
      "epoch 61, loss 2.937899351119995\n",
      "tensor(2.9363, grad_fn=<MseLossBackward0>)\n",
      "epoch 62, loss 2.936263084411621\n",
      "tensor(2.9347, grad_fn=<MseLossBackward0>)\n",
      "epoch 63, loss 2.9346866607666016\n",
      "tensor(2.9332, grad_fn=<MseLossBackward0>)\n",
      "epoch 64, loss 2.933168411254883\n",
      "tensor(2.9317, grad_fn=<MseLossBackward0>)\n",
      "epoch 65, loss 2.9317054748535156\n",
      "tensor(2.9303, grad_fn=<MseLossBackward0>)\n",
      "epoch 66, loss 2.9302961826324463\n",
      "tensor(2.9289, grad_fn=<MseLossBackward0>)\n",
      "epoch 67, loss 2.928938627243042\n",
      "tensor(2.9276, grad_fn=<MseLossBackward0>)\n",
      "epoch 68, loss 2.927630662918091\n",
      "tensor(2.9264, grad_fn=<MseLossBackward0>)\n",
      "epoch 69, loss 2.9263699054718018\n",
      "tensor(2.9252, grad_fn=<MseLossBackward0>)\n",
      "epoch 70, loss 2.9251551628112793\n",
      "tensor(2.9240, grad_fn=<MseLossBackward0>)\n",
      "epoch 71, loss 2.9239845275878906\n",
      "tensor(2.9229, grad_fn=<MseLossBackward0>)\n",
      "epoch 72, loss 2.922856569290161\n",
      "tensor(2.9218, grad_fn=<MseLossBackward0>)\n",
      "epoch 73, loss 2.921769618988037\n",
      "tensor(2.9207, grad_fn=<MseLossBackward0>)\n",
      "epoch 74, loss 2.9207215309143066\n",
      "tensor(2.9197, grad_fn=<MseLossBackward0>)\n",
      "epoch 75, loss 2.919710874557495\n",
      "tensor(2.9187, grad_fn=<MseLossBackward0>)\n",
      "epoch 76, loss 2.9187374114990234\n",
      "tensor(2.9178, grad_fn=<MseLossBackward0>)\n",
      "epoch 77, loss 2.9177982807159424\n",
      "tensor(2.9169, grad_fn=<MseLossBackward0>)\n",
      "epoch 78, loss 2.916893243789673\n",
      "tensor(2.9160, grad_fn=<MseLossBackward0>)\n",
      "epoch 79, loss 2.916020393371582\n",
      "tensor(2.9152, grad_fn=<MseLossBackward0>)\n",
      "epoch 80, loss 2.9151787757873535\n",
      "tensor(2.9144, grad_fn=<MseLossBackward0>)\n",
      "epoch 81, loss 2.9143669605255127\n",
      "tensor(2.9136, grad_fn=<MseLossBackward0>)\n",
      "epoch 82, loss 2.9135842323303223\n",
      "tensor(2.9128, grad_fn=<MseLossBackward0>)\n",
      "epoch 83, loss 2.9128291606903076\n",
      "tensor(2.9121, grad_fn=<MseLossBackward0>)\n",
      "epoch 84, loss 2.9121012687683105\n",
      "tensor(2.9114, grad_fn=<MseLossBackward0>)\n",
      "epoch 85, loss 2.911398410797119\n",
      "tensor(2.9107, grad_fn=<MseLossBackward0>)\n",
      "epoch 86, loss 2.9107208251953125\n",
      "tensor(2.9101, grad_fn=<MseLossBackward0>)\n",
      "epoch 87, loss 2.9100663661956787\n",
      "tensor(2.9094, grad_fn=<MseLossBackward0>)\n",
      "epoch 88, loss 2.909435510635376\n",
      "tensor(2.9088, grad_fn=<MseLossBackward0>)\n",
      "epoch 89, loss 2.9088263511657715\n",
      "tensor(2.9082, grad_fn=<MseLossBackward0>)\n",
      "epoch 90, loss 2.908238410949707\n",
      "tensor(2.9077, grad_fn=<MseLossBackward0>)\n",
      "epoch 91, loss 2.9076712131500244\n",
      "tensor(2.9071, grad_fn=<MseLossBackward0>)\n",
      "epoch 92, loss 2.907123327255249\n",
      "tensor(2.9066, grad_fn=<MseLossBackward0>)\n",
      "epoch 93, loss 2.90659499168396\n",
      "tensor(2.9061, grad_fn=<MseLossBackward0>)\n",
      "epoch 94, loss 2.9060840606689453\n",
      "tensor(2.9056, grad_fn=<MseLossBackward0>)\n",
      "epoch 95, loss 2.9055912494659424\n",
      "tensor(2.9051, grad_fn=<MseLossBackward0>)\n",
      "epoch 96, loss 2.9051153659820557\n",
      "tensor(2.9047, grad_fn=<MseLossBackward0>)\n",
      "epoch 97, loss 2.9046554565429688\n",
      "tensor(2.9042, grad_fn=<MseLossBackward0>)\n",
      "epoch 98, loss 2.9042110443115234\n",
      "tensor(2.9038, grad_fn=<MseLossBackward0>)\n",
      "epoch 99, loss 2.9037821292877197\n"
     ]
    }
   ],
   "source": [
    "# Training loop 1\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion1(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7108]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[3.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(model(inputs[:1]))\n",
    "print(labels[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 7\n",
    "output_dim = 1\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "criterion2 = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from graph_util.visualization_util import make_graph_visual\n",
    "from scheduling_util.modified_etf import Mod_ETF\n",
    "from scheduling_util.consolidated_functions import opt_schedule_given_ordering\n",
    "from scheduling_util.heuristics import native_rescheduler\n",
    "from graph_util.random_graph_functions import random_all_fork, random_all_join\n",
    "from graph_util.erdos_renyi_dag import er_dag\n",
    "from scheduling_util.approx_pseudosizes import speed_to_psize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "feature_id = ['in_degree', 'out_degree', 'num_descendants', 'longest_descendant_path', 'in_degree_betweenness_centrality', 'out_degree_betweenness_centrality', 'trophic_levels']    \n",
    "\n",
    "def get_feature_set(G, psize):\n",
    "    \n",
    "    lst = []\n",
    "    in_degree_lst = G.in_degree\n",
    "    out_degree_lst = G.out_degree\n",
    "    in_bet_lst = nx.algorithms.in_degree_centrality(G)\n",
    "    out_bet_lst = nx.algorithms.out_degree_centrality(G)\n",
    "    trophic_lst = nx.algorithms.trophic_levels(G)\n",
    "    for node in G.nodes:\n",
    "        in_degree = in_degree_lst[node]\n",
    "        out_degree = out_degree_lst[node]\n",
    "        in_bet = in_bet_lst[node]\n",
    "        # print(\"in betweennes\", in_bet)\n",
    "        out_bet = out_bet_lst[node]\n",
    "        trophic = trophic_lst[node]\n",
    "        # print(\"out_betweennes\", out_bet)\n",
    "        descendants = list(nx.algorithms.dag.descendants(G, node))\n",
    "        descendants.append(node)\n",
    "        num_descendants = len(descendants) \n",
    "        longest_descendant_path = nx.dag_longest_path_length(G.subgraph(descendants)) \n",
    "        lst.append(np.asarray([in_degree, out_degree, num_descendants, longest_descendant_path, in_bet, out_bet, trophic, psize[node]]))      \n",
    "    return np.asarray(lst)\n",
    "\n",
    "def compute_cost(w, t, s):\n",
    "    '''\n",
    "    Given weights w, time intervals t, and speeds s, compute the cost of the schedule; \n",
    "    returns total cost as well as separate power and time components.\n",
    "    '''\n",
    "    power = 0\n",
    "    time = 0\n",
    "    # print(t)\n",
    "    for j in range(len(s)):\n",
    "        if t[j] == -1:\n",
    "            return -1, -1, -1\n",
    "        power += w[j] * s[j]\n",
    "        time += t[j][1]\n",
    "    total_cost = power + time\n",
    "    return total_cost, power, time\n",
    "\n",
    "def psize_to_speed(psize):\n",
    "    return [np.sqrt(psize[i])for i in range(len(psize))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I is  1\n",
      "I is  2\n",
      "I is  3\n",
      "I is  4\n",
      "I is  5\n",
      "I is  6\n",
      "I is  7\n",
      "I is  8\n",
      "I is  9\n",
      "I is  10\n",
      "I is  11\n",
      "I is  12\n",
      "I is  13\n",
      "I is  14\n",
      "I is  15\n",
      "I is  16\n",
      "I is  17\n",
      "I is  18\n",
      "I is  19\n"
     ]
    }
   ],
   "source": [
    "# Make the testing set\n",
    "data = []\n",
    "lengths = []\n",
    "tie_breaking_rule = 2\n",
    "graphs = []\n",
    "x_tests = []\n",
    "x_test_labels = []\n",
    "greedy_objectives = []\n",
    "probability = 0.3\n",
    "\n",
    "for i in range(1, 20):\n",
    "    print(\"I is \", i)\n",
    "    for k in range(5):  \n",
    "        num_tasks = i + 5\n",
    "        num_machines = 3\n",
    "        w = [1 for _ in range(num_tasks)]\n",
    "        s = [1 for _ in range(num_tasks)]\n",
    "        p = [1 for _ in range(num_tasks)]\n",
    "        G, _ = er_dag(num_tasks, probability)\n",
    "        assert(nx.algorithms.dag.is_directed_acyclic_graph(G))\n",
    "        etf = Mod_ETF(G, w, s, num_machines, tie_breaking_rule, plot=False)\n",
    "        intervals, speeds, obj_value = opt_schedule_given_ordering(True, G, w, p, etf.order, plot=False, compare=False)\n",
    "        psize = speed_to_psize(speeds)\n",
    "        cost, power, time = compute_cost(w, intervals, speeds)\n",
    "        if cost != -1:\n",
    "            graphs.append(G)\n",
    "            greedy_objectives.append(np.asarray([cost, power, time]))\n",
    "            x_tests.append(np.asarray(get_feature_set(G, psize)))\n",
    "            x_test_labels.append(np.asarray(psize))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2.00001821, 2.00001821, 1.        , 1.        , 1.99998992,\n",
      "       1.        ]), array([2.9999972, 2.9999972, 4.       , 1.       , 1.       , 1.       ]), array([1.99993336, 2.00007478, 1.        , 1.        , 1.99996164,\n",
      "       1.        ]), array([2.5000037 , 1.99998992, 1.        , 2.5000037 , 1.        ,\n",
      "       1.        ]), array([3.00120976, 2.0004425 , 1.        , 1.9995374 , 2.99878489,\n",
      "       1.        ]), array([2.3333479 , 2.3333479 , 1.49998807, 2.3333479 , 1.49998807,\n",
      "       1.        , 1.        ]), array([3.50000489, 3.50000489, 5.00000904, 2.00001821, 1.        ,\n",
      "       1.99996164, 1.        ]), array([1.49998807, 4.        , 1.49998807, 2.9999972 , 1.        ,\n",
      "       1.99998992, 1.        ]), array([1.        , 2.9999972 , 2.9999972 , 1.99998992, 1.99998992,\n",
      "       1.99998992, 1.        ]), array([1.99998992, 1.        , 2.9999972 , 1.99998992, 1.99998992,\n",
      "       1.        , 1.        ]), array([2.999824  , 3.00017041, 1.99998992, 1.99976366, 1.        ,\n",
      "       1.        , 2.00024449, 1.        ]), array([3.50000489, 3.50000489, 1.        , 2.5000037 , 2.5000037 ,\n",
      "       1.99998992, 1.        , 1.        ]), array([2.9999972 , 2.9999972 , 2.9999972 , 1.        , 1.99998992,\n",
      "       1.        , 1.99998992, 1.        ]), array([5.00000904, 2.9999972 , 1.        , 1.99998992, 1.99998992,\n",
      "       2.9999972 , 1.        , 1.        ]), array([3.00082864, 2.99916588, 1.99998992, 2.00123462, 1.99877389,\n",
      "       1.        , 1.        , 1.        ]), array([4.        , 4.        , 2.9999972 , 2.9999972 , 1.        ,\n",
      "       1.99998992, 1.99998992, 1.99998992, 1.        ]), array([3.00003184, 1.99998992, 3.00003184, 2.00007478, 2.99992792,\n",
      "       1.        , 1.99993336, 1.        , 1.        ]), array([3.50007972, 1.99998992, 2.50003532, 1.        , 3.49993006,\n",
      "       1.99998992, 2.49994045, 1.        , 1.        ]), array([4.77326365, 1.99998992, 2.22675037, 3.7732674 , 2.7732574 ,\n",
      "       1.        , 1.        , 1.        , 1.22673346]), array([2.9999972 , 4.        , 1.99998992, 1.99998992, 2.9999972 ,\n",
      "       1.99998992, 1.        , 1.        , 1.        ]), array([3.27707747, 3.44584969, 3.27707747, 1.99998992, 2.55414735,\n",
      "       2.44587704, 1.99998992, 1.        , 1.        , 1.        ]), array([4.47495947, 2.9999972 , 4.47495947, 3.9499575 , 1.        ,\n",
      "       1.05005108, 1.99998992, 1.99998992, 1.        , 1.        ]), array([3.45599254, 3.08799785, 2.0879961 , 3.45599254, 2.9999972 ,\n",
      "       1.99998992, 1.91199756, 1.        , 1.        , 1.        ]), array([1.        , 3.27707747, 3.27707747, 3.44584969, 2.44587704,\n",
      "       1.99998992, 2.55414735, 1.99998992, 1.        , 1.        ]), array([3.33332655, 3.33332655, 3.33332655, 4.8477471 , 3.84775686,\n",
      "       1.        , 1.        , 1.07613652, 1.07613652, 1.        ]), array([5.50000994, 5.50000994, 3.19537075, 3.19537075, 2.6092264 ,\n",
      "       2.9999972 , 1.6092191 , 1.        , 1.99998992, 1.39077208,\n",
      "       1.        ]), array([3.66672712, 3.66668882, 2.66672166, 3.66657393, 2.66662368,\n",
      "       1.99998992, 1.        , 1.        , 2.66665634, 1.99998992,\n",
      "       1.        ]), array([6.25140008, 2.74859925, 3.74859938, 5.25138473, 4.25139037,\n",
      "       1.        , 1.74858308, 3.25142205, 1.        , 1.        ,\n",
      "       1.        ]), array([4.        , 2.9999972 , 4.        , 2.9999972 , 2.00001821,\n",
      "       1.99996164, 1.        , 1.99998992, 2.9999972 , 1.        ,\n",
      "       1.        ]), array([3.66665052, 2.66665634, 3.66665052, 3.66665052, 2.66665634,\n",
      "       2.66665634, 2.00001821, 1.99998992, 1.        , 1.        ,\n",
      "       1.        ]), array([4.        , 4.        , 4.        , 4.49999854, 4.49999854,\n",
      "       2.5000037 , 2.9999972 , 1.99998992, 2.5000037 , 1.99998992,\n",
      "       1.        , 1.        ]), array([5.39317018, 5.39317018, 1.21369882, 4.39317408, 2.5000037 ,\n",
      "       4.39317408, 1.50001256, 1.99998992, 2.5000037 , 1.        ,\n",
      "       1.49998807, 1.        ]), array([1.99998992, 5.00000904, 1.        , 5.00000904, 6.99999306,\n",
      "       6.00000126, 5.00000904, 1.        , 4.        , 1.49998807,\n",
      "       1.49998807, 1.        ]), array([6.00000126, 6.00000126, 5.00000904, 8.00001626, 6.99999306,\n",
      "       5.00000904, 2.00001821, 2.00001821, 1.        , 1.        ,\n",
      "       1.        , 1.99998992]), array([4.        , 4.        , 4.        , 2.9999972 , 2.9999972 ,\n",
      "       2.9999972 , 2.00001821, 1.99998992, 1.        , 1.        ,\n",
      "       1.99998992, 1.        ]), array([7.0671969 , 4.06719989, 4.93279658, 5.93278063, 6.06720645,\n",
      "       1.9663892 , 5.0672261 , 3.0671918 , 1.93279506, 1.9663892 ,\n",
      "       1.49998807, 1.49998807, 1.        ]), array([12.9999908 ,  6.00000126,  6.00000126,  5.00009849,  4.        ,\n",
      "        2.9999972 ,  2.9999972 ,  4.99987488,  1.99998992,  1.        ,\n",
      "        1.99998992,  1.        ,  1.        ]), array([5.26289481, 5.26289481, 4.2629035 , 4.2629035 , 3.44050562,\n",
      "       3.08529225, 1.91470871, 1.47418451, 2.47420316, 2.0852804 ,\n",
      "       1.99998992, 1.        , 1.        ]), array([2.29592195, 9.70409722, 8.70409307, 1.00002   , 2.9999972 ,\n",
      "       6.00000126, 2.9999972 , 1.99998992, 1.        , 1.        ,\n",
      "       1.        , 1.99998992, 1.99998992]), array([5.83724928, 4.83727238, 5.83724928, 1.32551472, 4.8372284 ,\n",
      "       1.        , 2.9999972 , 2.9999972 , 1.99998992, 1.99998992,\n",
      "       1.99998992, 1.        , 1.        ]), array([5.00000904, 5.00000904, 2.9999972 , 4.        , 2.00001821,\n",
      "       4.        , 2.9999972 , 4.        , 2.9999972 , 1.99998992,\n",
      "       1.99998992, 1.        , 1.        , 1.        ]), array([2.22675037, 6.99999306, 3.7732674 , 1.22673346, 6.00000126,\n",
      "       2.7732574 , 4.77326365, 5.00000904, 1.        , 4.        ,\n",
      "       2.9999972 , 1.99998992, 1.        , 1.        ]), array([2.9999972 , 4.49999854, 4.49999854, 1.99998992, 5.00000904,\n",
      "       4.        , 2.9999972 , 1.99998992, 2.9999972 , 4.        ,\n",
      "       1.99998992, 1.        , 1.        , 1.        ]), array([4.49999854, 4.        , 5.00000904, 4.49999854, 2.9999972 ,\n",
      "       1.99998992, 4.        , 2.9999972 , 1.        , 2.9999972 ,\n",
      "       1.99998992, 1.        , 1.        , 1.99998992]), array([4.49999854, 4.49999854, 5.00000904, 1.99998992, 6.99999306,\n",
      "       1.99998992, 4.        , 2.9999972 , 1.99998992, 2.9999972 ,\n",
      "       1.        , 2.9999972 , 1.        , 1.        ]), array([7.49998473, 6.50000124, 7.49998473, 9.        , 8.00001626,\n",
      "       6.99999306, 6.50000124, 1.        , 1.        , 3.00017041,\n",
      "       2.999824  , 1.        , 1.        , 1.99998992, 1.        ]), array([6.72556796, 6.72556796, 5.72558755, 3.81475586, 5.72558755,\n",
      "       3.81475586, 1.54885492, 4.61274415, 2.37046054, 3.61277453,\n",
      "       1.99998992, 1.37046825, 1.01676972, 1.        , 1.        ]), array([14.00001956, 10.99996822, 10.0000148 ,  1.        ,  9.        ,\n",
      "        1.        ,  8.00001626,  6.99999306,  6.00000126,  1.99998992,\n",
      "        2.5000037 ,  2.5000037 ,  1.        ,  1.        ,  1.        ]), array([ 7.49998473,  6.50000124, 10.99996822,  7.49998473,  6.50000124,\n",
      "        1.4937484 ,  6.50627455,  5.50624997,  4.        ,  1.        ,\n",
      "        1.99998992,  1.        ,  2.9999972 ,  1.99998992,  1.        ]), array([5.34307848, 5.34307848, 4.31380438, 4.29256242, 4.39363521,\n",
      "       1.        , 3.14175625, 3.393664  , 3.4645866 , 4.85823314,\n",
      "       5.31380873, 3.85823878, 1.14175636, 1.49998807, 1.49998807,\n",
      "       1.        ]), array([7.16006619, 7.16006619, 6.31818496, 1.0910847 , 1.67984929,\n",
      "       4.90697813, 4.00196024, 6.00196101, 3.90694756, 3.00003184,\n",
      "       5.00193225, 2.9999972 , 1.        , 1.99998992, 1.        ,\n",
      "       1.        ]), array([4.92507056, 5.53745611, 5.53745611, 4.53745382, 3.92507419,\n",
      "       4.53745382, 7.07490842, 6.07494327, 1.925073  , 2.9999972 ,\n",
      "       2.9999972 , 1.        , 1.        , 1.        , 1.99998992,\n",
      "       1.        ]), array([8.00001626, 4.        , 8.00001626, 5.00000904, 4.49999854,\n",
      "       6.99999306, 2.9999972 , 2.9999972 , 1.99998992, 4.49999854,\n",
      "       2.9999972 , 1.        , 2.00001821, 1.        , 1.        ,\n",
      "       1.99998992]), array([3.33332655, 5.33332836, 4.33339162, 4.33326672, 5.33328217,\n",
      "       2.00001821, 5.33337455, 4.33334999, 3.33336306, 3.33332655,\n",
      "       2.9999972 , 1.        , 1.99998992, 1.99996164, 1.        ,\n",
      "       1.        ]), array([8.00001626, 8.00001626, 6.99999306, 6.99999306, 1.        ,\n",
      "       6.00000126, 6.00000126, 9.        , 4.        , 4.        ,\n",
      "       2.9999972 , 1.        , 2.9999972 , 2.9999972 , 1.99998992,\n",
      "       1.        , 1.        ]), array([ 4.8317755 , 12.16823689,  2.16825625,  9.        ,  8.00001626,\n",
      "        4.        ,  3.8317671 ,  6.99999306,  2.9999972 ,  2.9999972 ,\n",
      "        1.99998992,  1.99998992,  1.99998992,  1.        ,  1.        ,\n",
      "        1.        ,  2.9999972 ]), array([8.91482249, 4.04259278, 3.04258249, 4.04255257, 7.91482569,\n",
      "       3.04258249, 6.91484875, 3.15694717, 3.15694717, 2.15693282,\n",
      "       3.6861312 , 2.15693282, 2.68612432, 1.68612822, 2.3138669 ,\n",
      "       1.        , 1.        ]), array([ 6.00000126,  6.00000126,  6.00000126,  7.49998473,  7.49998473,\n",
      "        4.33334999,  4.33334999,  4.33334999, 10.0000148 ,  9.        ,\n",
      "        5.47040643,  2.52959482,  4.47043364,  1.5295753 ,  3.47043367,\n",
      "        1.99998992,  1.        ,  1.        ]), array([14.5026919 , 13.50268516,  3.49731141,  5.00000904,  3.99996   ,\n",
      "        5.00000904,  1.99998992,  4.99996432,  4.        ,  2.9999972 ,\n",
      "        4.        ,  2.9999972 ,  2.9999972 ,  4.        ,  2.9999972 ,\n",
      "        1.99998992,  1.        ,  1.        ]), array([ 7.27062689,  5.36468979,  6.27061697, 12.9999908 ,  5.36468979,\n",
      "        8.7293657 , 11.99998881,  4.00016   ,  3.99984   ,  3.00010113,\n",
      "        2.9999972 ,  2.99989328,  1.99998992,  2.9999972 ,  1.        ,\n",
      "        1.        ,  1.99998992,  1.        ]), array([6.00000126, 5.00000904, 6.00000126, 4.        , 5.00000904,\n",
      "       6.00000126, 4.        , 2.9999972 , 1.99998992, 4.99996432,\n",
      "       2.9999972 , 4.        , 2.9999972 , 1.99998992, 1.        ,\n",
      "       1.99998992, 1.        , 1.        ]), array([17.00003607, 16.        , 14.99997408, 14.00001956, 10.49701681,\n",
      "        9.49699816,  8.4969918 ,  2.50297713,  2.9999972 ,  1.50297792,\n",
      "        4.49702918,  1.        ,  2.9999972 ,  1.99998992,  1.99998992,\n",
      "        1.        ,  1.        ,  1.        ]), array([7.00004598, 6.00005025, 5.00000904, 5.99995227, 6.99994015,\n",
      "       5.00000904, 4.00008   , 4.        , 5.00000904, 2.9999972 ,\n",
      "       1.99998992, 4.        , 2.9999972 , 1.99998992, 3.99992   ,\n",
      "       1.        , 1.        , 1.        , 1.99998992]), array([6.29212073, 6.29227123, 5.29220823, 6.41563307, 5.29211621,\n",
      "       5.41562712, 4.        , 4.4156298 , 2.9999972 , 1.99998992,\n",
      "       4.58435203, 3.50000489, 2.5000037 , 1.        , 3.50000489,\n",
      "       2.5000037 , 1.        , 1.99998992, 1.        ]), array([17.84749864,  8.42375162,  6.6955185 ,  8.42375162,  1.15248813,\n",
      "        8.15199573,  5.29883569,  5.69547771,  4.69549895,  4.29882169,\n",
      "        6.9943323 ,  5.99431979,  2.00001821,  1.99998992,  1.00566801,\n",
      "        1.        ,  2.00567909,  1.        ,  1.        ]), array([4.33330836, 6.33332589, 6.33332589, 6.33332589, 5.33337455,\n",
      "       3.33332655, 5.33332836, 4.33334999, 4.33334999, 5.33332836,\n",
      "       3.33332655, 3.33332655, 2.9999972 , 1.99998992, 1.        ,\n",
      "       1.99998992, 1.99998992, 1.        , 1.        ]), array([5.3317119 , 5.61405636, 6.33332589, 1.7719337 , 6.33544002,\n",
      "       5.33494506, 6.33121212, 5.33332836, 5.61405636, 9.22804657,\n",
      "       5.00000904, 4.        , 2.9999972 , 1.        , 2.9999972 ,\n",
      "       1.99998992, 1.99998992, 1.        , 1.        ]), array([6.30276088, 5.30279573, 7.30280362, 6.34858573, 6.34863612,\n",
      "       5.34858129, 5.34862754, 7.1740944 , 5.23842234, 5.23842234,\n",
      "       7.7713828 , 1.2285949 , 1.52315091, 3.3857104 , 4.        ,\n",
      "       3.3857104 , 1.        , 2.9999972 , 1.99998992, 1.        ]), array([6.99369759, 1.99998992, 6.99369759, 4.8702752 , 6.01259824,\n",
      "       4.01260992, 7.11715684, 3.87026929, 6.11711396, 5.11714165,\n",
      "       3.01258506, 2.87027976, 4.11712506, 5.01258365, 1.        ,\n",
      "       3.11713149, 1.88287795, 1.49998807, 1.49998807, 1.        ]), array([6.66667236, 5.66668503, 6.66667236, 6.99999306, 5.66663742,\n",
      "       6.66667236, 5.66668503, 6.00000126, 5.00000904, 5.00000904,\n",
      "       6.99999306, 6.00000126, 2.66665634, 2.66665634, 2.66665634,\n",
      "       1.99998992, 2.9999972 , 1.99998992, 1.        , 1.        ]), array([5.98159523, 7.03681729, 8.0368281 , 5.98159523, 8.07548623,\n",
      "       1.88768364, 7.0754936 , 5.63996002, 6.036849  , 6.36008049,\n",
      "       4.6399314 , 3.63992978, 5.36005843, 3.3600723 , 2.63994254,\n",
      "       1.        , 2.36006406, 1.        , 1.        , 1.        ]), array([ 6.63278818, 10.15167927,  7.63278756,  4.73441026,  5.73439652,\n",
      "        7.63278756,  6.63278818,  9.15171403,  3.73440165,  2.9999972 ,\n",
      "       10.88610635,  4.        ,  1.09148167,  4.90852887,  1.11389027,\n",
      "        3.908529  ,  1.        ,  1.99998992,  1.        ,  1.        ,\n",
      "        1.99998992]), array([6.84408385, 5.8440645 , 6.84408385, 6.31185228, 7.3118864 ,\n",
      "       4.84409286, 5.31182647, 4.31185225, 5.8440645 , 3.31185402,\n",
      "       4.84409286, 4.68813434, 1.99998992, 1.99998992, 2.9999972 ,\n",
      "       3.68816661, 2.9999972 , 1.        , 1.        , 1.99998992,\n",
      "       1.        ]), array([10.49999774,  7.60490929, 10.49999774,  6.6049    ,  7.93002496,\n",
      "        6.9300036 ,  3.46510779,  5.93000423,  4.92999853,  3.92998906,\n",
      "        2.4651198 ,  2.92998536,  5.60486685,  5.07001779,  4.06998346,\n",
      "        3.06999458,  1.99998992,  1.        ,  1.99998992,  1.        ,\n",
      "        1.        ]), array([6.99999306, 6.99999306, 6.99999306, 7.32828213, 3.34341225,\n",
      "       7.32828213, 6.32829367, 6.32829367, 5.32829506, 5.32829506,\n",
      "       2.9999972 , 2.34344049, 2.9999972 , 4.        , 1.        ,\n",
      "       2.9999972 , 1.99998992, 1.99998992, 1.99998992, 1.        ,\n",
      "       1.        ]), array([ 1.22062723,  9.88970414,  9.88970414,  8.88970003, 15.77940563,\n",
      "        2.9999972 ,  5.00000904,  4.9999196 ,  4.00004   ,  4.        ,\n",
      "        2.9999972 ,  2.9999972 ,  3.99996   ,  8.88970003,  1.99998992,\n",
      "        5.00005377,  1.99993336,  2.00004649,  1.        ,  1.        ,\n",
      "        1.        ]), array([7.3335348 , 6.3331749 , 6.33332589, 7.33315568, 8.00007283,\n",
      "       7.33331816, 7.99990313, 7.00009889, 6.00009924, 5.99990328,\n",
      "       6.33347689, 6.99994015, 4.        , 1.99990507, 2.9999972 ,\n",
      "       3.00006648, 2.99992792, 1.99998992, 1.        , 2.00007478,\n",
      "       1.        , 1.        ]), array([7.38056756, 7.38056756, 7.23884406, 6.12859536, 5.63255289,\n",
      "       6.07543623, 6.23885497, 6.63253064, 4.29206519, 3.29204736,\n",
      "       4.85395805, 4.85395805, 6.27287088, 5.27285591, 2.2920446 ,\n",
      "       2.00001821, 1.99998992, 1.        , 1.4350842 , 1.        ,\n",
      "       1.99998992, 1.        ]), array([9.83556227, 8.8355779 , 8.8355779 , 2.32885912, 7.61401242,\n",
      "       9.83556227, 6.61405237, 7.61406761, 6.61405237, 1.77190708,\n",
      "       4.        , 4.        , 4.        , 2.9999972 , 2.9999972 ,\n",
      "       1.        , 2.9999972 , 2.9999972 , 1.        , 1.99998992,\n",
      "       1.99998992, 1.        ]), array([ 7.33331816,  7.33331816,  6.87289142,  7.33331816,  6.87289142,\n",
      "       11.74576838,  3.2542356 ,  4.        ,  5.25422668,  6.74575951,\n",
      "        5.74575282,  6.99999306,  2.9999972 ,  4.25423626,  2.9999972 ,\n",
      "        2.9999972 ,  1.99998992,  1.        ,  1.99998992,  1.99998992,\n",
      "        1.99998992,  1.        ]), array([12.43070152, 11.43075528,  7.59052091,  2.81497928,  6.59051584,\n",
      "        5.59052922,  8.59451309,  7.59448876,  9.59450625,  2.59451556,\n",
      "        6.40550543,  2.97876533,  4.        ,  5.4054855 ,  4.40550714,\n",
      "        1.59451231,  2.9999972 ,  3.40550116,  1.99998992,  1.99998992,\n",
      "        1.        ,  1.        ,  1.        ]), array([7.66664108, 7.66664108, 7.66675183, 5.66663742, 6.66656908,\n",
      "       6.66677564, 5.66668503, 6.50000124, 6.66667236, 6.50000124,\n",
      "       5.50005685, 5.49996304, 4.49995612, 2.9999972 , 5.66668503,\n",
      "       4.50004097, 2.9999972 , 1.        , 1.99998992, 1.        ,\n",
      "       1.99998992, 1.        , 1.        ]), array([18.50204196, 17.5020906 ,  5.66668503,  4.49791989,  6.66667236,\n",
      "        5.66668503,  4.66668006,  6.66662072,  6.66667236,  4.66663686,\n",
      "        4.66668006,  3.66665052,  3.66665052,  3.66668882,  5.66663742,\n",
      "        1.99998992,  2.9999972 ,  2.9999972 ,  1.99998992,  1.99998992,\n",
      "        1.99998992,  1.        ,  1.        ]), array([12.05277032, 11.05276568,  7.51505465,  9.94727444, 10.947231  ,\n",
      "        8.94727744,  2.53771272, 14.46227658,  6.63108851,  5.63108154,\n",
      "        8.36892827,  7.36894458,  3.18447594,  1.99998992,  4.63106096,\n",
      "        1.        ,  3.18447594,  2.9999972 ,  2.9999972 ,  2.00001821,\n",
      "        1.99998992,  1.        ,  1.        ]), array([ 7.82684552,  7.82684552,  7.34632237, 13.65369011,  6.34631826,\n",
      "        8.70409307,  7.70406638,  3.94956002,  6.70411199,  3.82683581,\n",
      "        5.34636133,  2.9999972 ,  3.34633849,  4.34634934,  2.9495658 ,\n",
      "        3.82683581,  2.9999972 ,  1.        ,  1.99998992,  1.99998992,\n",
      "        2.00001821,  1.        ,  1.        ]), array([ 8.00001626, 10.49999774,  8.00001626,  8.00001626,  9.50001848,\n",
      "       10.49999774,  9.50001848,  5.2493225 ,  5.2493225 ,  6.50137805,\n",
      "        1.64845057,  5.50137025,  6.35155925,  6.85020694,  2.9999972 ,\n",
      "        1.99998992,  1.99998992,  4.        ,  2.9999972 ,  1.99998992,\n",
      "        1.        ,  2.9999972 ,  1.        ,  1.        ]), array([11.99998881, 11.99998881,  7.35358806,  7.29275426,  3.50000489,\n",
      "        6.24460117,  7.35358806,  6.29277293,  6.46262831,  5.53736199,\n",
      "        4.53736861,  5.00000904,  5.46264407,  2.5000037 ,  4.        ,\n",
      "        4.46265625,  3.50000489,  1.99998992,  2.5000037 ,  1.        ,\n",
      "        2.9999972 ,  1.49998807,  1.49998807,  1.        ]), array([11.99998881, 11.99998881, 10.99996822, 10.99996822,  8.78156176,\n",
      "        8.78156176, 15.5631039 ,  2.4368771 , 12.5631258 ,  1.99998992,\n",
      "       11.56312821, 10.56315001,  1.43688169,  5.00000904,  5.00000904,\n",
      "        4.33447416,  1.        ,  2.66554602,  1.99998992,  1.        ,\n",
      "        3.66553999,  1.        ,  2.33447841,  1.        ]), array([11.99998881, 21.00003946, 20.00003618, 11.99998881,  9.50051164,\n",
      "        8.49996532,  9.49952534,  8.50002363,  7.4996561 ,  1.        ,\n",
      "        4.3658013 ,  5.36580162,  7.50036814,  6.42739115,  5.4274089 ,\n",
      "        4.42740514,  1.20679013,  1.        ,  1.71371663,  3.57259142,\n",
      "        1.71371663,  2.57259144,  1.99998992,  1.        ]), array([11.95285414,  2.85910899,  8.18801056, 10.95285644,  5.90601785,\n",
      "        9.18799157,  5.90601785,  7.18797548,  4.9060035 ,  6.1880045 ,\n",
      "        4.9060035 ,  3.99884008,  5.00000904,  4.00116008,  2.9999972 ,\n",
      "        3.00117511,  4.        ,  1.99998992,  1.99982022,  2.00018792,\n",
      "        1.        ,  2.99881952,  1.        ,  1.        ])]\n"
     ]
    }
   ],
   "source": [
    "#x_tests[:2]\n",
    "print(x_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "np.save('test_data', np.asarray(x_tests, dtype='object'))\n",
    "np.save('test_labels', np.asarray(x_test_labels,dtype='object'))\n",
    "np.save('test_objectives', np.asarray(greedy_objectives,dtype='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0a38c7fd2505b15b46fab770b43c894ec907a53509f636050b5c66c47dc2015"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('caltech_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
